{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2fe479-dd02-4590-937b-771a3d3c0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = \"https://books.toscrape.com/\"\n",
    "# Refactoring\n",
    "# 여러 개의 코드로 나누어서 작성 / 관리\n",
    "def parse_book(card, category) : \n",
    "    title = card.h3.a[\"title\"].strip()\n",
    "    product_url = urljoin(BASE, card.h3.a[\"href\"])\n",
    "    price_text = card.select_one(\".price_color\").get_text(strip=True)\n",
    "    price_num = re.sub(r'[^0-9\\.]',\"\",price_text)\n",
    "    stock_text = card.select_one(\".availability\").get_text(strip=True)\n",
    "    stock = \"In stock\" in stock_text\n",
    "    rating_word = card.select_one(\".star-rating\")[\"class\"][1]\n",
    "    rating_map = {\"One\":1,\"Two\":2,\"Three\":3,\"Four\":4,\"Five\":5}\n",
    "    rating=rating_map.get(rating_word)\n",
    "\n",
    "    return {\n",
    "        \"title\":title,\n",
    "        \"price\":price,\n",
    "        \"stock\":stock,\n",
    "        \"rating\":rating,\n",
    "        \"category\":category,\n",
    "        \"product_url\":product_url\n",
    "    }\n",
    "def crawl_category(cat_url, max_pages=2) :\n",
    "    data = []\n",
    "    next_url = cat_url\n",
    "    for _ in range(max_pages) :\n",
    "        html = requests.get(next_url, timeout=10).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        category = soup.select_one(\".page-header.action h1\")\n",
    "        category = category.get_text(strip=True) if category else \"Unknown\"\n",
    "        for card in soup.select(\"ol.row > li.col-xs-6.col-sm-4.col-md-3.col-lg-3\") :\n",
    "            data.append()\n",
    "        nxt = soup.select_one(\"li.next a\")\n",
    "        time.sleep(0.5)\n",
    "        if not nxt :\n",
    "            \n",
    "    \n",
    "    return data\n",
    "\n",
    "def crawl_few_categories() :\n",
    "    # test01 = requests.get(BASE)\n",
    "    # test02 = requests.get(BASE, timeout=10).text\n",
    "    soup = BeautifulSoup(requests.get(BASE, timeout=10).text, \"html.parser\")\n",
    "    cat_links = [urljoin(BASE, a[\"href\"]) for a in soup.select(\".side_categories > ul > li > ul > li > a\")[:3]]\n",
    "\n",
    "    all_rows = []\n",
    "    for link in cat_links :\n",
    "        all_rows.extend(crawl_category(link))\n",
    "\n",
    "if __name__ == \"__main__\"\n",
    "rows = crawl_few_categories()\n",
    "print(\"출력\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a91ecd-d74c-44d6-b548-4fb005bdfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = \"https://books.toscrape.com/\"\n",
    "\n",
    "# 책 1권 정보 파싱\n",
    "def parse_book(card, category): \n",
    "    title = card.h3.a[\"title\"].strip()\n",
    "    product_url = urljoin(BASE, card.h3.a[\"href\"])\n",
    "    price_text = card.select_one(\".price_color\").get_text(strip=True)\n",
    "    price = float(re.sub(r'[^0-9\\.]', \"\", price_text))  # 숫자만 추출\n",
    "    stock_text = card.select_one(\".availability\").get_text(strip=True)\n",
    "    stock = \"In stock\" in stock_text\n",
    "    rating_word = card.select_one(\".star-rating\")[\"class\"][1]\n",
    "    rating_map = {\"One\":1, \"Two\":2, \"Three\":3, \"Four\":4, \"Five\":5}\n",
    "    rating = rating_map.get(rating_word)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price\": price,\n",
    "        \"stock\": stock,\n",
    "        \"rating\": rating,\n",
    "        \"category\": category,\n",
    "        \"product_url\": product_url\n",
    "    }\n",
    "\n",
    "# 카테고리 크롤링\n",
    "def crawl_category(cat_url, max_pages=2):\n",
    "    data = []\n",
    "    next_url = cat_url\n",
    "    for _ in range(max_pages):\n",
    "        html = requests.get(next_url, timeout=10).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        category = soup.select_one(\".page-header.action h1\")\n",
    "        category = category.get_text(strip=True) if category else \"Unknown\"\n",
    "\n",
    "        for card in soup.select(\"ol.row > li.col-xs-6.col-sm-4.col-md-3.col-lg-3\"):\n",
    "            data.append(parse_book(card, category))\n",
    "\n",
    "        nxt = soup.select_one(\"li.next a\")\n",
    "        if nxt:\n",
    "            next_url = urljoin(next_url, nxt[\"href\"])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.5)  # 요청 간격\n",
    "\n",
    "    return data\n",
    "\n",
    "# 여러 카테고리 크롤링 (테스트용 3개만)\n",
    "def crawl_few_categories():\n",
    "    soup = BeautifulSoup(requests.get(BASE, timeout=10).text, \"html.parser\")\n",
    "    cat_links = [urljoin(BASE, a[\"href\"]) for a in soup.select(\".side_categories > ul > li > ul > li > a\")[:3]]\n",
    "\n",
    "    all_rows = []\n",
    "    for link in cat_links:\n",
    "        all_rows.extend(crawl_category(link))\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "# 실행부\n",
    "if __name__ == \"__main__\":\n",
    "    rows = crawl_few_categories()\n",
    "    print(f\"Collected : {len(rows)} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
